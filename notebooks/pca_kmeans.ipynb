{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "1. Create environment:\n",
    "    In terminal, run:\n",
    "    \n",
    "    `conda env create -n caiman_test_env -f studio/app/optinist/wrappers/caiman/conda/caiman.yaml`\n",
    "\n",
    "    `conda activate caiman_test_env`\n",
    "\n",
    "2. Install some additional packages:\n",
    "\n",
    "   `pip install pynwb imageio ipython jupyter notebook \"pydantic<2.0.0\" python-dotenv uvicorn xmltodict plotly scikit-image opencv-python`\n",
    "  - If running in VS code, you may need to restart and/or select the correct environment with \"Python: Select Interpreter\"\n",
    "\n",
    "3. Run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run caiman on sample data to get labelimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "\n",
    "# Import OptiNiSt core data modules\n",
    "from studio.app.dir_path import DIRPATH\n",
    "from studio.app.common.dataclass import ImageData\n",
    "from studio.app.optinist.dataclass import FluoData\n",
    "# Import ROI detection modules\n",
    "from studio.app.optinist.wrappers.caiman import motion_correction, cnmf\n",
    "# Import OptiNiSt analysis modules\n",
    "from studio.app.optinist.wrappers.optinist.dimension_reduction.pca import PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create input directories based on default saving path\n",
    "input_dir = os.path.join(DIRPATH.INPUT_DIR, \"1\")\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "unique_id = str(uuid.uuid4())[:8]  # Generate 8-char unique ID\n",
    "\n",
    "# Input file path\n",
    "input_file = os.path.join(input_dir, \"sample_mouse2p_image.tiff\")\n",
    "sample_data = ImageData([input_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion correction\n",
    "\n",
    "# Set parameters\n",
    "motion_correction_params = {\n",
    "    'border_nan': 'copy', \n",
    "    'gSig_filt': None, \n",
    "    'is3D': False, \n",
    "    'max_deviation_rigid': 3, \n",
    "    'max_shifts': [6, 6], \n",
    "    'min_mov': None, \n",
    "    'niter_rig': 1, \n",
    "    'nonneg_movie': True, \n",
    "    'num_frames_split': 80, \n",
    "    'num_splits_to_process_els': None, \n",
    "    'num_splits_to_process_rig': None, \n",
    "    'overlaps': [32, 32], \n",
    "    'pw_rigid': False, \n",
    "    'shifts_opencv': True, \n",
    "    'splits_els': 14, \n",
    "    'splits_rig': 14, \n",
    "    'strides': [96, 96], \n",
    "    'upsample_factor_grid': 4, \n",
    "    'use_cuda': False\n",
    "}\n",
    "\n",
    "# Create output directory for motion correction\n",
    "mc_function_id = f\"caiman_mc_{unique_id}\"\n",
    "mc_output_dir = os.path.join(DIRPATH.OUTPUT_DIR, \"1\", unique_id, mc_function_id)\n",
    "os.makedirs(mc_output_dir, exist_ok=True)\n",
    "\n",
    "# Perform motion correction\n",
    "ret_mc = motion_correction.caiman_mc(sample_data, mc_output_dir, motion_correction_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNMF roi detection\n",
    "# Set parameters\n",
    "caiman_cnmf_params = {\n",
    "    'p': 2,\n",
    "    'nb': 2,\n",
    "    'merge_thr': 0.85,\n",
    "    'stride': 6,\n",
    "    'K': 10,\n",
    "    'gSig': [4, 4], \n",
    "    'method_init': 'greedy_roi',\n",
    "    'ssub': 2,\n",
    "    'tsub': 2,\n",
    "    'roi_thr': 0.9,\n",
    "    'do_refit': False,\n",
    "    'use_online': False,\n",
    "    'use_cnn': False,\n",
    "}\n",
    "\n",
    "\n",
    "# Create output directory for CNMF\n",
    "cnmf_function_id = f\"caiman_cnmf_{unique_id}\"\n",
    "cnmf_output_dir = os.path.join(DIRPATH.OUTPUT_DIR, \"1\", unique_id, cnmf_function_id)\n",
    "os.makedirs(cnmf_output_dir, exist_ok=True)\n",
    "\n",
    "# Run CNMF for ROI detection\n",
    "cnmf_info = cnmf.caiman_cnmf(ret_mc['mc_images'], cnmf_output_dir, caiman_cnmf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used in batch processing (adjusted slightly for notebook format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import sparse\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import convolve\n",
    "from skimage import measure\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis Section\n",
    "def pca_analysis(fluorescence, roi_masks, params=None):\n",
    "    \"\"\"Perform PCA analysis similar to pca_analysis.py module\"\"\"\n",
    "    # Get data shape\n",
    "    n_cells = fluorescence.shape[0]\n",
    "    print(f\"PCA will use {n_cells} cells\")\n",
    "    \n",
    "    # Check if we have enough ROIs for PCA\n",
    "    if n_cells < 2:\n",
    "        print(\"Not enough cells for PCA analysis (minimum 2 required)\")\n",
    "        # Create dummy placeholders\n",
    "        dummy_scores = np.zeros((1, 1))\n",
    "        dummy_components = np.zeros((1, 1))\n",
    "        dummy_explained_variance = np.zeros(1)\n",
    "        \n",
    "        return {\n",
    "            'scores': dummy_scores,\n",
    "            'components': dummy_components,\n",
    "            'explained_variance': dummy_explained_variance,\n",
    "            'has_sufficient_data': False\n",
    "        }\n",
    "    \n",
    "    # Set default parameters if none provided\n",
    "    if params is None:\n",
    "        params = {\"n_components\": min(50, n_cells), \"standard_norm\": True}\n",
    "    \n",
    "    # Prepare data\n",
    "    if params.get(\"standard_norm\", True):\n",
    "        # Center the data\n",
    "        data = fluorescence - np.mean(fluorescence, axis=1, keepdims=True)\n",
    "        # Scale to unit variance\n",
    "        std_values = np.std(data, axis=1, keepdims=True)\n",
    "        # Avoid division by zero\n",
    "        std_values[std_values == 0] = 1.0\n",
    "        data = data / std_values\n",
    "    else:\n",
    "        data = fluorescence\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=params[\"n_components\"])\n",
    "    scores = pca.fit_transform(data.T)  # time x components\n",
    "    components = pca.components_  # components x cells\n",
    "    explained_variance = pca.explained_variance_ratio_ * 100\n",
    "    \n",
    "    return {\n",
    "        'scores': scores,\n",
    "        'components': components,\n",
    "        'explained_variance': explained_variance,\n",
    "        'has_sufficient_data': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Analysis Section\n",
    "def kmeans_analysis(fluorescence, roi_masks, params=None):\n",
    "    \"\"\"Perform KMeans analysis similar to kmeans_analysis.py module\"\"\"\n",
    "    # Get data shape\n",
    "    n_cells = fluorescence.shape[0]\n",
    "    print(f\"KMeans will use {n_cells} cells\")\n",
    "    \n",
    "    # Set default parameters if none provided\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Ensure n_clusters exists and doesn't exceed the number of cells\n",
    "    params[\"n_clusters\"] = min(params.get(\"n_clusters\", 3), n_cells)\n",
    "    \n",
    "    # Handle case when there are insufficient cells for clustering\n",
    "    if n_cells < 2:\n",
    "        print(\"Not enough cells for KMeans clustering (minimum 2 required)\")\n",
    "        # Set dummy values\n",
    "        cluster_labels = np.zeros(max(1, n_cells), dtype=int)\n",
    "        corr_matrix = np.ones((max(1, n_cells), max(1, n_cells)), dtype=float)\n",
    "        \n",
    "        return {\n",
    "            'labels': cluster_labels,\n",
    "            'corr_matrix': corr_matrix,\n",
    "            'has_sufficient_data': False\n",
    "        }\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(fluorescence)\n",
    "    \n",
    "    # Determine optimal number of clusters using silhouette score\n",
    "    k_range = range(2, min(21, n_cells))  # Test cluster numbers from 2 to 20 (or max cells)\n",
    "    silhouette_values = []\n",
    "    \n",
    "    # Skip silhouette computation if we have too few cells\n",
    "    if n_cells > 3:\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "            labels = kmeans.fit_predict(corr_matrix)\n",
    "            silhouette_values.append(silhouette_score(corr_matrix, labels))\n",
    "        \n",
    "        # Choose k with highest silhouette score\n",
    "        best_k_idx = np.argmax(silhouette_values)\n",
    "        k_optimal = k_range[best_k_idx]\n",
    "    else:\n",
    "        k_optimal = min(n_cells, params[\"n_clusters\"])\n",
    "    \n",
    "    # Perform clustering with optimal k\n",
    "    kmeans = KMeans(n_clusters=k_optimal, init='k-means++', n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(corr_matrix)\n",
    "    \n",
    "    return {\n",
    "        'labels': cluster_labels,\n",
    "        'corr_matrix': corr_matrix,\n",
    "        'silhouette_values': silhouette_values if n_cells > 3 else None,\n",
    "        'k_optimal': k_optimal,\n",
    "        'has_sufficient_data': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pca_visualization(scores, explained_variance, components, roi_masks, output_dir):\n",
    "    \"\"\"Generate PCA visualization with separate files for each component\"\"\"\n",
    "    # Check if inputs are valid\n",
    "    if components is None or scores is None:\n",
    "        print(\"Warning: Missing PCA components or scores\")\n",
    "        return\n",
    "    \n",
    "    # Handle the case of insufficient ROIs - create error images\n",
    "    is_data_insufficient = (\n",
    "        components.shape[0] < 2 or \n",
    "        scores.shape[1] < 2 or \n",
    "        np.allclose(components, 0, atol=1e-7) or \n",
    "        np.allclose(scores, 0, atol=1e-7) or \n",
    "        np.all(np.isnan(components)) or \n",
    "        np.all(np.isnan(scores))\n",
    "    )\n",
    "    \n",
    "    if is_data_insufficient:\n",
    "        # Create error image for variance plot\n",
    "        plt.figure()\n",
    "        plt.text(\n",
    "            0.5, 0.5,\n",
    "            \"Insufficient ROIs for PCA analysis.\\nAt least 2 ROIs required.\",\n",
    "            ha=\"center\", va=\"center\",\n",
    "            transform=plt.gca().transAxes\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        variance_path = os.path.join(output_dir, \"pca_analysis_variance.png\")\n",
    "        plt.savefig(variance_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Similar error images for other plots\n",
    "        scatter_path = os.path.join(output_dir, \"pca_analysis.png\")\n",
    "        contrib_path = os.path.join(output_dir, \"pca_contribution.png\")\n",
    "        spatial_path = os.path.join(output_dir, \"pca_component_1_spatial.png\")\n",
    "        time_path = os.path.join(output_dir, \"pca_component_1_time.png\")\n",
    "        \n",
    "        for path in [scatter_path, contrib_path, spatial_path, time_path]:\n",
    "            plt.figure()\n",
    "            plt.text(\n",
    "                0.5, 0.5,\n",
    "                \"Insufficient ROIs for PCA analysis.\\nAt least 2 ROIs required.\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                transform=plt.gca().transAxes\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(path, bbox_inches=\"tight\")\n",
    "            plt.close()        \n",
    "        return\n",
    "    \n",
    "    # Number of components to visualize\n",
    "    num_components = min(50, components.shape[0], scores.shape[1])\n",
    "    plots_to_show = 10  # Set to 10 as too many make legend illegible\n",
    "    \n",
    "    # 1. Plot explained variance\n",
    "    plt.figure()\n",
    "    num_display = min(plots_to_show, len(explained_variance))\n",
    "    plt.bar(range(1, num_display + 1), explained_variance[:num_display])\n",
    "    plt.title(\"Explained Variance\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.ylabel(\"Explained Variance (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    variance_path = os.path.join(output_dir, \"pca_analysis_variance.png\")\n",
    "    plt.savefig(variance_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Create PCA scatter plot (first 2-3 components)\n",
    "    plt.figure()\n",
    "    if scores.shape[1] >= 2:\n",
    "        plt.scatter(scores[:, 0], scores[:, 1], alpha=0.7)\n",
    "        plt.xlabel(\"PC 1\")\n",
    "        plt.ylabel(\"PC 2\")\n",
    "        \n",
    "        if scores.shape[1] >= 3:\n",
    "            plt.figure()\n",
    "            plt.scatter(\n",
    "                scores[:, 0], scores[:, 1], \n",
    "                c=scores[:, 2], cmap=\"viridis\", alpha=0.7\n",
    "            )\n",
    "            plt.colorbar(label=\"PC 3\")\n",
    "            plt.xlabel(\"PC 1\")\n",
    "            plt.ylabel(\"PC 2\")\n",
    "    \n",
    "    scatter_path = os.path.join(output_dir, \"pca_analysis.png\")\n",
    "    plt.savefig(scatter_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. For each component, create time course and spatial map\n",
    "    for i in range(num_components):\n",
    "        # Time course\n",
    "        plt.figure()\n",
    "        plt.plot(scores[:, i], linewidth=2)\n",
    "        plt.title(f\"PC {i+1} Time Course\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Component Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        time_path = os.path.join(output_dir, f\"pca_component_{i+1}_time.png\")\n",
    "        plt.savefig(time_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Spatial map - attempt only if roi_masks has appropriate shape\n",
    "        component_weights = components[i]  # Using actual weights, not absolute values\n",
    "        \n",
    "        # Create spatial component maps\n",
    "        if roi_masks is not None and hasattr(roi_masks, \"shape\"):\n",
    "            try:\n",
    "                # Extract valid cell IDs (non-NaN values) from roi_masks\n",
    "                non_nan_mask = (\n",
    "                    ~np.isnan(roi_masks)\n",
    "                    if np.any(np.isnan(roi_masks))\n",
    "                    else np.ones_like(roi_masks, dtype=bool)\n",
    "                )\n",
    "                \n",
    "                if np.any(non_nan_mask):\n",
    "                    # Create component map\n",
    "                    component_map = np.full_like(roi_masks, np.nan)\n",
    "                    \n",
    "                    # Get unique cell IDs\n",
    "                    valid_ids = np.unique(roi_masks[non_nan_mask])\n",
    "                    valid_ids = np.sort(valid_ids)\n",
    "                    \n",
    "                    # Map each cell's weight to its spatial location\n",
    "                    for idx, cell_id in enumerate(valid_ids):\n",
    "                        if idx < len(component_weights):\n",
    "                            # Find pixels for this cell and assign component weight\n",
    "                            cell_mask = np.isclose(roi_masks, cell_id)\n",
    "                            if np.any(cell_mask):\n",
    "                                component_map[cell_mask] = component_weights[idx]\n",
    "                    \n",
    "                    # Check if map has valid data\n",
    "                    if not np.all(np.isnan(component_map)):\n",
    "                        # Use symmetric divergent colormap with consistent scaling\n",
    "                        vmax = np.nanmax(np.abs(component_map))\n",
    "                        \n",
    "                        plt.figure()\n",
    "                        im = plt.imshow(component_map, cmap=\"RdBu_r\", vmin=-vmax, vmax=vmax)\n",
    "                        plt.colorbar(im, label=\"Component Weight\")\n",
    "                        plt.title(f\"PC {i+1} Spatial Map\")\n",
    "                        \n",
    "                        spatial_path = os.path.join(output_dir, f\"pca_component_{i+1}_spatial.png\")\n",
    "                        plt.savefig(spatial_path, bbox_inches=\"tight\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        raise ValueError(\"No valid values in component map\")\n",
    "                else:\n",
    "                    raise ValueError(\"No non-NaN values found in ROI mask\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating spatial map for PC {i+1}: {str(e)}\")\n",
    "                \n",
    "                # Create fallback visualization\n",
    "                plt.figure()\n",
    "                plt.bar(range(len(component_weights)), component_weights)\n",
    "                plt.title(f\"PC {i+1} Component Weights\")\n",
    "                plt.xlabel(\"Cell Index\")\n",
    "                plt.ylabel(\"Weight\")\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                spatial_path = os.path.join(output_dir, f\"pca_component_{i+1}_spatial.png\")\n",
    "                plt.savefig(spatial_path, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "        else:\n",
    "            # Create alternative visualization using direct component values\n",
    "            plt.figure()\n",
    "            plt.bar(range(len(component_weights)), component_weights)\n",
    "            plt.title(f\"PC {i+1} Component Weights\")\n",
    "            plt.xlabel(\"Cell Index\")\n",
    "            plt.ylabel(\"Weight\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            spatial_path = os.path.join(output_dir, f\"pca_component_{i+1}_spatial.png\")\n",
    "            plt.savefig(spatial_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "    \n",
    "    # 5. Save the contribution weights as a separate visualization\n",
    "    plt.figure()\n",
    "    top_n = min(plots_to_show, components.shape[0])\n",
    "    for i in range(top_n):\n",
    "        plt.bar(\n",
    "            range(len(components[i])),\n",
    "            components[i],  # Using actual weights, not absolute values\n",
    "            alpha=0.7,\n",
    "            label=f\"PC {i+1}\",\n",
    "        )\n",
    "    plt.xlabel(\"Cell Index\")\n",
    "    plt.ylabel(\"Component Weight\")\n",
    "    plt.title(\"PCA Component Contributions\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    contrib_path = os.path.join(output_dir, \"pca_contribution.png\")\n",
    "    plt.savefig(contrib_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kmeans_visualization(labels, corr_matrix, fluorescence, roi_masks, output_dir):\n",
    "    \"\"\"Generate KMeans visualizations with separate files for each component\"\"\"\n",
    "    if labels is None or len(labels) == 0:\n",
    "        print(\"Warning: Missing cluster labels\")\n",
    "        return\n",
    "    \n",
    "    # Handle the case of insufficient ROIs\n",
    "    is_data_insufficient = (\n",
    "        labels is None\n",
    "        or len(labels) < 2\n",
    "        or corr_matrix is None\n",
    "        or corr_matrix.shape[0] < 2\n",
    "    )\n",
    "    \n",
    "    if is_data_insufficient:\n",
    "        # Create error image for correlation matrix plot\n",
    "        plt.figure()\n",
    "        plt.text(\n",
    "            0.5, 0.5,\n",
    "            \"Insufficient ROIs for k-means clustering.\\nAt least 2 ROIs required.\",\n",
    "            ha=\"center\", va=\"center\",\n",
    "            transform=plt.gca().transAxes\n",
    "        )\n",
    "        plt.axis(\"off\")\n",
    "        matrix_path = os.path.join(output_dir, \"clustering_analysis.png\")\n",
    "        plt.savefig(matrix_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Create error images for other plots\n",
    "        time_path = os.path.join(output_dir, \"cluster_time_courses.png\")\n",
    "        map_path = os.path.join(output_dir, \"cluster_spatial_map.png\")\n",
    "        \n",
    "        for path in [time_path, map_path]:\n",
    "            plt.figure()\n",
    "            plt.text(\n",
    "                0.5, 0.5,\n",
    "                \"Insufficient ROIs for k-means clustering.\\nAt least 2 ROIs required.\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                transform=plt.gca().transAxes\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Reorder correlation matrix based on clusters\n",
    "    sort_idx = np.argsort(labels)\n",
    "    sorted_corr_matrix = corr_matrix[sort_idx][:, sort_idx]\n",
    "    \n",
    "    # Calculate cluster information\n",
    "    unique_clusters = np.unique(labels)\n",
    "    n_clusters = len(unique_clusters)\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n_clusters))\n",
    "    custom_cmap = ListedColormap(colors)\n",
    "    \n",
    "    # 1. Correlation matrix heatmap\n",
    "    plt.figure()\n",
    "    im = plt.imshow(sorted_corr_matrix, cmap=\"jet\")\n",
    "    plt.colorbar(im)\n",
    "    plt.title(f\"K-means Clustering (k={n_clusters})\")\n",
    "    plt.xlabel(\"Cells\")\n",
    "    plt.ylabel(\"Cells\")\n",
    "    \n",
    "    matrix_path = os.path.join(output_dir, \"clustering_analysis.png\")\n",
    "    plt.savefig(matrix_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Mean time courses by cluster\n",
    "    if fluorescence is not None and fluorescence.shape[0] >= len(labels):\n",
    "        plt.figure()\n",
    "        cluster_averages = []\n",
    "        \n",
    "        for i, cluster in enumerate(unique_clusters):\n",
    "            cluster_mask = labels == cluster\n",
    "            if np.any(cluster_mask):\n",
    "                cluster_avg = np.mean(fluorescence[cluster_mask], axis=0)\n",
    "                plt.plot(\n",
    "                    cluster_avg, color=colors[i], linewidth=2, label=f\"Cluster {i+1}\"\n",
    "                )\n",
    "                cluster_averages.append(cluster_avg)\n",
    "        \n",
    "        plt.title(\"Mean Time Course by Cluster\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Fluorescence\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        time_path = os.path.join(output_dir, \"cluster_time_courses.png\")\n",
    "        plt.savefig(time_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Spatial cluster map - attempt only if roi_masks has appropriate shape\n",
    "    if roi_masks is not None and hasattr(roi_masks, \"shape\"):\n",
    "        try:\n",
    "            # Create cluster colormap\n",
    "            unique_clusters = np.unique(labels)\n",
    "            n_clusters = len(unique_clusters)\n",
    "            colors = plt.cm.jet(np.linspace(0, 1, n_clusters))\n",
    "            custom_cmap = ListedColormap(colors)\n",
    "            \n",
    "            # Check for 3D mask (standard case with multiple ROIs)\n",
    "            if len(roi_masks.shape) == 3:\n",
    "                cluster_map = np.zeros(roi_masks.shape[:2])\n",
    "                \n",
    "                # Create cluster map\n",
    "                for i, label in enumerate(labels):\n",
    "                    if i < roi_masks.shape[2]:\n",
    "                        roi_mask = roi_masks[:, :, i]\n",
    "                        cluster_map[roi_mask > 0] = (\n",
    "                            label + 1\n",
    "                        )  # +1 to avoid 0 (background)\n",
    "                \n",
    "                # Create a mask of all cell locations\n",
    "                all_cells_mask = np.zeros(roi_masks.shape[:2], dtype=bool)\n",
    "                for i in range(roi_masks.shape[2]):\n",
    "                    all_cells_mask |= roi_masks[:, :, i] > 0\n",
    "                \n",
    "                # Create masked cluster map for better visualization\n",
    "                masked_cluster_map = np.ma.masked_array(\n",
    "                    cluster_map,\n",
    "                    mask=~all_cells_mask,  # Mask background (non-cell areas)\n",
    "                )\n",
    "                \n",
    "                # Plot cluster map\n",
    "                plt.figure()\n",
    "                im = plt.imshow(\n",
    "                    masked_cluster_map, cmap=custom_cmap, interpolation=\"nearest\"\n",
    "                )\n",
    "                \n",
    "                # Add colorbar with cluster labels\n",
    "                colorbar = plt.colorbar(im, ticks=np.arange(1, n_clusters + 1))\n",
    "                colorbar.set_label(\"Cluster\")\n",
    "                \n",
    "                # Add cluster legend with unique colors\n",
    "                handles = [\n",
    "                    plt.Rectangle((0, 0), 1, 1, color=colors[i])\n",
    "                    for i in range(n_clusters)\n",
    "                ]\n",
    "                plt.legend(\n",
    "                    handles,\n",
    "                    [f\"Cluster {i+1}\" for i in range(n_clusters)],\n",
    "                    loc=\"upper right\",\n",
    "                    bbox_to_anchor=(1.3, 1),\n",
    "                )\n",
    "                \n",
    "                plt.title(\"Cluster Spatial Map\")\n",
    "                \n",
    "                # Save maps\n",
    "                map_path = os.path.join(output_dir, \"cluster_spatial_map.png\")\n",
    "                plt.savefig(map_path, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                \n",
    "            # Simpler 2D mask case\n",
    "            elif len(roi_masks.shape) == 2:\n",
    "                cluster_map = np.zeros(roi_masks.shape)\n",
    "                # Use most common cluster for the mask\n",
    "                if len(labels) > 0:\n",
    "                    counts = np.bincount(labels)\n",
    "                    most_common = np.argmax(counts) if len(counts) > 0 else 0\n",
    "                    cluster_map[roi_masks > 0] = most_common + 1\n",
    "                \n",
    "                # Plot and save as above\n",
    "                plt.figure()\n",
    "                im = plt.imshow(cluster_map, cmap=custom_cmap)\n",
    "                plt.colorbar(im, label=\"Cluster\")\n",
    "                plt.title(\"Cluster Assignments\")\n",
    "                \n",
    "                map_path = os.path.join(output_dir, \"cluster_spatial_map.png\")\n",
    "                plt.savefig(map_path, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create cluster spatial map: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your own caiman data produced using caiman.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_or = # Add path to your cnmf output data here\n",
    "\n",
    "roi_masks = cnmf_info[\"cell_roi\"].data  \n",
    "timecourse = cnmf_info[\"fluorescence\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "roi_masks = cnmf_info[\"cell_roi\"].data  \n",
    "timecourse = cnmf_info[\"fluorescence\"].data\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"./test_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Run PCA Analysis\n",
    "print(\"\\n===== Running PCA Analysis =====\")\n",
    "pca_results = pca_analysis(timecourse, roi_masks)\n",
    "if pca_results['has_sufficient_data']:\n",
    "    generate_pca_visualization(\n",
    "        pca_results['scores'], \n",
    "        pca_results['explained_variance'], \n",
    "        pca_results['components'], \n",
    "        roi_masks, \n",
    "        output_dir\n",
    "    )\n",
    "    print(\"PCA analysis and visualization completed.\")\n",
    "else:\n",
    "    print(\"PCA analysis skipped due to insufficient data.\")\n",
    "\n",
    "# Run KMeans Analysis\n",
    "print(\"\\n===== Running KMeans Analysis =====\")\n",
    "kmeans_results = kmeans_analysis(timecourse, roi_masks)\n",
    "if kmeans_results['has_sufficient_data']:\n",
    "    generate_kmeans_visualization(\n",
    "        kmeans_results['labels'], \n",
    "        kmeans_results['corr_matrix'], \n",
    "        timecourse, \n",
    "        roi_masks, \n",
    "        output_dir\n",
    "    )\n",
    "    print(\"KMeans analysis and visualization completed.\")\n",
    "else:\n",
    "    print(\"KMeans analysis skipped due to insufficient data.\")\n",
    "\n",
    "print(\"\\nAll analyses completed. Results saved to:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
